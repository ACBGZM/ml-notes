

| <span style="white-space:nowrap;">状态&emsp;</span> | <span style="white-space:nowrap;">paper&emsp;&emsp;</span>   | <span style="white-space:nowrap;">时间&emsp;&emsp;</span> | <span style="white-space:nowrap;">机构&emsp;&emsp;</span> | <span style="white-space:nowrap;">发表&emsp;&emsp;</span> |
| --------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------- | --------------------------------------------------------- | --------------------------------------------------------- |
|                                                     | DeepFakes and Beyond A Survey of Face Manipulation and Fake Detection | 2020.01                                                   |                                                           | arxiv                                                     |
|                                                     | A Review on Face Reenactment Techniques                      | 2020.02                                                   |                                                           |                                                           |
|                                                     | The Creation and Detection of Deepfakes A Survey             | 2020.04                                                   |                                                           | arxiv                                                     |
|                                                     | What comprises a good talking-head video generation A Survey and Benchmark | 2020.05                                                   | University of Rochester                                   | arxiv                                                     |

​	



### 论文列表
- [ ] 1.2020-01 [arxiv] DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection
- [ ] 2.2020-02 A Review on Face Reenactment Techniques
- [ ] 3.2020-04 [arxiv] The Creation and Detection of Deepfakes: A Survey
- [ ] 4.2020-05 [arxiv] What comprises a good talking-head video generation? A Survey and Benchmark





### 2. A Review on Face Reenactment Techniques

现有的面部重现方法有两个主要局限性，首先，它们需要庞大的图像数据集来创建逼真的面部模型，其次，如果面部图像在训练数据集中不可用，则不能很好地推广。 新面部重现的生成需要大的图像数据集，并且训练这些模型需要花费几个小时。 

深度学习方面的一些进展已显示出使用生成对抗网络（GAN）的相当可观的结果。  GAN的最新工作通过引入few-shot learning（小样本学习）解决了训练数据集太大的问题。 

本文回顾了使用few-shot learning进行人脸重现的现有方法，以及其他人脸重现的方法。



#### Ⅰ简介

Face Re-enactment任务：source face的表情和姿态被转移到target face上。

- source face：提供表情和姿态
- target face：提供身份，外貌细节

**第一个困难：数据集。**

在面部识别、情感分类、面部关键点检测等方面的发展，促进了使用深度学习的新方法的产生。

深度学习方法的质量取决于数据量。这种系统往往需要long video of target face，但实际中，有时需要从少量target的图像来完成。

很多系统引入了 few-shot，帮助从少量target face图片创建合成的人脸。

**第二个困难：不同单元的合成。**如眼睛，头发，鼻子形状，以及其他部分如衣服，背景，遮挡，等。人类对这些错误很敏感。

GAN的技术已经给出了解决这个问题的方法（参考文献19 Image quality assessment: From error visibility to structural similarity.）。



### Ⅱ 数据集

LFW dataset[1]包含用其名称标记的、裁剪和重新缩放过的面部图像。 同一个人的面部图像配对在一起。

Annotated Facial Landmarks in the Wild dataset[2]提供了从Flickr（雅虎旗下图片网站）手动标记21个面部关键点获得的面部图像。Helen dataset[3]建立在使用Flickr上的各种关键字搜索（例如“婚礼”，“男孩”，“室外”，“工作室”）获得的大量照片上。 手动准确定位了下巴，嘴巴，眼睛和眉毛。

CelebA dataset[4]由二十万张图像组成。  CelebA中的每个图像都带有40个面部属性的注释。IARPA Janus Benchmark–C dataset[5]包含3,531个对象的面部图像和视频，每个对象都有近6个图像和三个视频。由Amazon Mechanical Turk（AMT）工作人员标记了面部的属性和完全遮挡的区域。

VGGFace2 dataset[6]包含330万张各种名人的图像，这些图像是从Google Image下载的。（与VGGNet是同一个小组的工作。）VGGFace2提供注释，用于估计不同姿势和年龄的面部匹配。FaceForensics dataset[7]从Youtube获得，其中包含1004个视频，其中视频包含标签“ face”，“ news program”等。Face2Face方法用于从真实图像创建合成图像。

VoxCeleb [8]包含超过100,000次露面的1,251位名人，这些都是从Youtube视频中提取的。  VoxCeleb2 [9]大约有100万视频音频片段，来自六千名名人，提取自YouTube。



### Ⅲ 方法

#### A. 物理/光流方法（建立脸部模型+驱动）

T. Akimoto等 [10]在两张图片（人脸的正面和侧面图片）上进行特征提取，收集创建通用模型所需的特征。使用模板匹配技术（template-matching technique）提取下巴尖，嘴巴，鼻梁等特征。Oleg Alexander等[11]使用高分辨率的面部扫描**创建了一个真实的面部模型**。 通过对演员的三十七次高分辨率扫描，构建了一个数字角色。 在此，使用同一演员的视频来驱动数字模型的面部动画。 在包含演员的眼睛，嘴唇，牙齿和前额的基础网格上进行渲染，以获得最终的面部模型。

Y.C.  Lee等 [12]通过**使用图像分析技术扫描对象获得了面部网格**。 基于物理的面部建模可在通用面部网格上对面部表情进行动画处理。F. Pighin等人在文献[13]中使用相机捕获具有特定面部表情的人脸的各种视图。 在脸上手动标记点，然后将这些点用于恢复不同特征的3D位置。 这些3D位置用于生成通用3D面网格。 从照片中提取纹理图，然后将其映射到面部网格上。以上过程是针对几种不同的面部表情进行的。

V.Blanz等 [14]使用数据集中的人脸构建了人脸模型，数据集中每个人脸都由形状和纹理矢量（分别为Si和Ti）编码。 新形状Snew和纹理Tnew表示为数据集中人脸的Si和Ti的线性组合，其中a和b为参数化系数。 可以通过更改a和b参数来创建新的形状和纹理。 为了获得3D表面和给定图像之间的最大相似性，必须找到给定图像具有最大后验概率的参数（a，b）。

Borshukov G.等  [15]为 《The Matrix Reloaded》 **创建了演员的面部渲染**。 以100微米的分辨率扫描演员面部的石膏模型。 在不同的照明条件下拍摄了演员的照片。 演员脸上的光照反射存储在2D光照图中，用于在皮肤中创建地下散射的外观。

Averbuch-Elor等  [20]提出了一种**扭曲系统**（warping system），其中**从视频中提取面部特征和非面部特征并将其扭曲在相应的目标图像上**。 隐藏的区域（如嘴内部和牙齿）将转移到动画的面部。 然后生成诸如皱纹，折痕和照明的精细细节，以实现逼真的面部表情。

>“扭曲系统”，从视频中提取面部、非面部特征，通过扭曲系统扭曲到目标图像上。然后进行皱纹等细节的处理。
>
>物理、光流法，思路都在如何建立更好的人脸模型上，然后将人脸动作通过关键点迁移过去；而到了最后这个工作，提出了迁移动作的方法。



#### B.深度学习方法

Justus Thies 等 [21]提出了一种人脸重现方法，其中**基于模型的绑定被用于重现目标演员的形状身份**。**通过精确跟踪源面部，然后使用子空间变形（subspace deformations）来传递表情**，同时考虑目标视频中的背景照明，从而重新渲染目标的面部。 使用嘴部合成方法生成逼真的嘴部形状和内部。		Ting-Chun Wang等  [22]提出了pix2pixHD，其中使用conditional GANs和多尺度鉴别器（multi-scale discriminators）将语义标签映射转换为逼真的图像。

Olivia Wiles 等在[23]中提出了一种基于网络的方法来生成帧。嵌入网络（embedding network）是一种U-Net架构[24]，用于学习从source帧到称为嵌入面的表示帧的映射。 驱动网络（driving network）是编码器-解码器网络架构，其获取driving视频的帧，并转换embedded的面部以生成生成的帧。

面部重演方法需要大量人的面部图像来创建他们的重演。Zhang 等 [25]提出了一种一次性学习器（one-shot learner），其中使用了两个单独的编码器，其中一个用于使用target图像创建面部外观的潜在空间（latent space），另一个source图像创建形状空间（shape space）。 然后，解码器组合这些编码的特征以创建重新制定的面孔。形状编码器（shape encoder, E）是在WFLW数据集上训练的边界编码器，然后在训练外观自动编码器和解码器时将其冻结。 外观自动编码器（appearance encoder, F）从target图像创建特征，并将这些特征提供给外观自动编码器中的解码器以重建目标图像。 然后，将这些重构图像中的多层特征与自适应解码器D中的特征进行组合。解码器D是一种受U-Net [24]启发的结构，没有编码器，但具有多尺度SPADE块[26]，以避免语义信息丢失。

Zakharov等 [27]提出了一个基于GAN的系统[28]，该系统首先在VoxCeleb [8，9]数据集上执行元学习（meta-learning），然后可以做到仅用几张图像就可以创建没见过的人的面部模型。 元学习阶段用来获取正确的参数，然后根据用户的输入图像进行微调，以实现逼真的面部重现。

Nirkin等 [31]提出了FSGAN，它是一种基于主题不可知的基于GAN的方法，具有四个生成器和多规格判别器。 重演生成器（reenactment generator）使用target图像的表达式生成source图像的重演图像，而分段生成器（segmentation generator）计算target的蒙版（S）。修补网络（inpainting network）基于分割蒙版S绘制重演图像的缺失部分。混合生成器（blending generator）将重演图像混合到target图像中。

Ting-Chun Wang 等 [29]提出了 few-shot vid2vid，以通过引入 few-shot 和泛化能力来克服vid2vid [30]的局限性。 Few-shot vid2vid模型使用conditional GAN [31]框架，该框架将一个视频和一些图像作为输入，输入的这些图像用于动态决定网络的权重以生成不可见域的视频。



### Ⅳ 评估指标

余弦相似度（CSIM）[16]用于人脸识别，以计算**两个特征向量之间的相似度**。

$$CSIM = cos(θ)=(u v)/(| u | | v |)$$

> 计算两个向量 u，v 夹角的余弦值。计算相似度。



Fréchet Inception Distance（Fréchet起始距离，FID） [17] 衡量**合成图像样本的质量**。 两个多元高斯变量之间的FID给出为：

$$FID = || μ_r−μ_g || ^ 2 + Tr(Σ_r+ Σ_g−2(Σ_rΣ_g)^{1/2})$$

其中，生成的图像和实像是两个高斯图像，其中（μr，Σr）和（μg，Σg）是两个高斯图像的均值和协方差 。

> $\mu$ ：图像的均值。$\Sigma$：图像的协方差。
>
> Tr 指的是被称为「迹」的线性代数运算（即方阵主对角线上的元素之和）。
>
> FID是评估GAN网络的常用属性。计算真实图像和生成图像的特征向量之间距离的一种度量。分数越低代表两组图像越相似，或者说二者的统计量越相似，FID 在最佳情况下的得分为 0.0，表示两组图像相同。



结构相似度（SSIM）[18]测量参考图像和已处理图像之间的感知差异。  SSIM告知结构信息的变化。

> 也是评估图像的相似度，很早出现2004。



### Ⅴ 其他工作

















## 3.The Creation and Detection of Deepfake : A Survey

### 3 技术背景

大部分deepfakes模型都是由**生成式网络**、**编码器解码器网络**的变体或组合产生的。

#### 3.1 神经网络

某些deepfake模型使用 one-shot 或 few-shot 学习技术，该技术可使预训练的模型适应与训练集 $X$ 相似的新数据集 $X'$ 。对此两种常见的方法是：

- 在 $X$ 上进行训练时，在前向传播过程中将 $x' ∈ X ’$ 的信息传递到神经网络内层

- 对 $X'$ 的一部分样本进行行一些额外的训练迭代

#### 3.2 损失函数

为了使用梯度下降优化算法来更新权重，需要确定损失函数。不同的损失函数应用到不同的学习目标。如：

- 如果模型是n分类器，就用交叉熵损失函数 $L_{CE}$。
- 其他deepfake常用的损失函数，如 $L_1,L2$ 范数。在图像存在较大差异时效果很差。
- 比较两个不同的图像，一种方法是让它们通过另一个网络（perceptual model），并测量层激活值（feature maps，特征图）的差异，称为感知损失 $L_{perc}$。通常使用人脸识别网络（如VGGFACE）计算这个损失。
- 跟 $L_{perc}$ 思路相近，$L_{FM}$ 使用网络的最终输出。利用感知模型最后一层所捕获到的高级语义信息。
- 常见的内容损失函数 $L_C$ ，用于帮助生成器根据感知模型创建现实的特征。 在 $L_C$ 中，只有 $x_g$ 经过感知模型，并且测量网络的特征图之间的差异。

####  3.3 GAN（for deepfakes）

deepfakes模型一般由六个模型组合或变形而成。

- 编码器-解码器网络（ED）。  ED由至少两个网络组成，一个编码器En和一个解码器De。ED的中心位置较窄，因此当训练 $De(En(x)) = x_g$ 时，网络将输入图片信息进行编码，然后再进行解码，输出新的图片。  $En(x)= e$ 通常被称为编码或嵌入， $E = En(X)$ 被称为“隐空间”。

  - 自动编码器：如果一个编码器和一个解码器是对称的，并且网络以 $De(En(x))= x$ 为目标，则该网络称为自动编码器，并且输出是x的重建，表示为 $\hat x$。

    > 输入的数据经过神经网络降维到一个编码(code)，接着又通过另外一个神经网络去解码得到一个与输入原数据一模一样的生成数据，然后通过去比较这两个数据，最小化他们之间的差异来训练这个网络中编码器和解码器的参数。当这个过程训练完之后，我们可以拿出这个解码器，随机传入一个编码(code)，希望通过解码器能够生成一个和原数据差不多的数据

  - 变分自动编码器（VAE）：编码器可以学习给定X的解码器的后验分布。VAE比自动编码器具有更好的生成内容，这是因为潜在空间中的概念是不纠缠的，因此编码对插值和修饰有更好的响应。

    > 在编码过程给它增加一些限制，迫使其生成的隐含向量能够粗略的遵循一个标准正态分布。这样我们生成一张新图片就很简单了，只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片先编码。

- 卷积神经网络（CNN）。与全连接网络相比，CNN可以在数据中学习空间层次结构，从而可以更有效地处理图像。卷积层学习filters的参数，filters进行卷积运算，从而形成一个抽象特征映射作为输出。随着网络变得越来越深，使用池化层来降低维数，并使用上采样层来增加维数。 通过卷积，池化和上采样层，可以构建用于图像的ED CNN。

- 生成对抗网络（GAN）。GAN由两个相互对抗的神经网络组成：生成器 $G$ 和判别器 $D$。$G$ 用假想的 $D$ 创建假样本 $x_g$。$D$ 学习区分真实样本 $x∈X$ 和假样本 $x_g= G(z)$。 

  > 具体来说，有分别用于训练D和G的对抗损失：
  >
  > $L_{adv}(D)= max logD(x) + log(1-D(G(z)))$ 
  >
  > $L_{adv}(G)= min log(1-D(G(z)))$ 
  >
  > 这个过程，G学习如何生成与原始分布无法区分的样本。 在使用中，丢弃D并使用G生成内容。在图像生成任务中，此方法可以生成逼真的图像。	
  - 图像到图像转换（pix2pix）。G：输入给定视觉上下文 $x_c$ ，生成图像 $x_g$；D：在 $(x, x_c)$ 与 $(x_g, x_c)$ 之间进行区分。此外，G是具有从En到 De 的跳边连接的ED CNN（称为 U-Net架构），这使G可以在需要时通过绕过压缩层来生成高保真度的图像。

    > 生成器是ED CNN，根据上下文进行生成；判别器根据上下文进行区分。
    >
    > pix2pix架构使从一个图像域到另一个图像域的成对翻译成为可能[72]。

  - CycleGAN。网络形成一个由两个GAN组成的循环，用于将图像从一个域转换为另一个域，然后再次返回。通过循环一致性损失 $L_{cyc}$ 确保一致性。

    > 是pix2pix的改进，可通过不成对的训练来进行图像翻译[192]。 

- 循环神经网络（RNN）。可以处理顺序和可变长度数据的神经网络类型。网络在处理完 $x^{(i-1)}$ 后，记住内部状态并用来处理 $x ^{(i)}$。 在deepfake，RNN经常用来处理音频，也有时处理视频。RNN的更多高级版本包括长短期内存（LSTM）和门递归单元（GRU）。

#### 3.4 特征表示

大多数deepfake架构使用某种形式的中间表示来捕获，有时会操纵s和t的面部结构，姿势和表情。 一种方法是使用面部动作编码系统（FACS）并分别测量面部的分类动作单元（AU）[43]。 另一种方法是使用单眼重建从2D图像中获得头部的3D可变形模型（3DMM），其中，姿势和表情由一组矢量和矩阵参数化。 然后使用参数或头部本身的3D渲染。有些人使用头部或身体的UV贴图使网络更好地了解形状。

另一种方法是使用图像分段来帮助网络分离不同的概念（面部、头发等）。最常用的是地标（也称为关键点），它是可以使用开源CV库有效地跟踪的在脸部或身体上定义的位置的集合。 地标通常以2D图像的形式呈现给网络，每个地标都具有高斯点。 一些作品按通道分隔地标，以使网络更易于标识和关联它们。 同样，也可以使用面部边界和身体骨骼。

对于音频（语音），最常用的方法是分割音频输入，然后针对每个段，测量“Mel-倒谱系数”（MCC），以捕获占主导地位的语音频率。

- 图片
  - 使用中间形式来捕获面部结构。
    - 面部动作编码系统FACS，测量动作单元AU。
    - 使用单眼重建，从2D图像建立头部的3D可变形模型（3DMM）；姿势和表情由一组矢量和矩阵参数化， 然后使用参数或头部本身的3D渲染。（有人使用UV贴图更好地理解形状）
  - 把图像分割来分离不同的概念（面部、头发等）。最常用的是地标（也称为关键点），它是可以使用开源CV库有效地跟踪的在脸部或身体上定义的位置的集合。
- 音频
  - 最常用的方法是分割音频输入，然后对于每一段，测量梅尔倒谱系数（MCC），以捕获占主导地位的语音频率。

#### 3.5 deepfake 创建基础

pipeline【图】



   （1）让网络直接在图像上工作并自己执行映射。
   （2）训练ED网络以从表情中解开身份，然后在将目标传递通过解码器之前修改/交换目标的编码。
   （3）在将附加编码传递给解码器之前，添加附加编码（例如，AU或嵌入）。
   （4）在生成之前将中间的面部/身体表示转换为所需的身份/表达式（例如，使用次级网络或使用所需表达式的目标对象的3D模型转换边界）。
   （5）使用源视频中后续帧中的光学流来驱动发生器。
   （6）使用3D渲染，扭曲图像或生成的内容的组合来创建原始内容（头发，场景等）的合成，然后将该合成通过另一个网络（例如pix2pix）以反映真实感。

#### 3.7 挑战

- 概括。 生成网络是数据驱动的，因此必须在其输出中完善训练数据。 要求特定身份的大量高质量图像。 而且，通常比victim更容易获得driver的大型数据集。 结果，在过去的几年中，研究人员努力工作以最大程度地减少所需的训练数据量，并能够在新的目标和来源身份（训练期间未见）上执行训练后的模型。
- 配对训练。 人工神经网络专家向每个给定输入表示模型的期望输出。 在训练多个身份和行为时，数据配对的过程很费力，有时甚至是不切实际的。 为避免此问题，许多Deepfake网络要么（1）通过使用从t的相同视频中选择的帧来训练自我监督的方式，要么（2）使用不成对的网络（如Cycle-GAN），或者（3）利用ED网络的编码。
- 身份泄漏。 有时，驾驶员的身份（例如，重新制定中的s）会部分转移到xд。 当在单个输入身份上进行训练时，或者在对网络进行多个身份训练但同时使用相同身份进行数据配对时，会发生Tis。 研究人员提出的一些解决方案包括注意机制，短时学习，解缠结，边界转换以及AdaIN或跳过连接，以将相关信息传递给生成器。
- 遮挡物。 用手，头发，眼镜或其他任何物品阻塞x s或x t的地方。 另一类阻塞是可能隐藏或动态变化的眼睛和嘴巴区域。 结果，出现伪像，例如裁剪的图像或不一致的面部特征。 为了减轻这种情况，诸如[121、128、145]之类的作品对受阻区域进行了分割和修复。
- 时间连贯性。 深假视频可以产生更多明显的伪像，例如晃动和晃动[164]。 这是因为大多数Deepfake网络在没有先前帧的上下文的情况下单独处理每个帧。 为了减轻这种情况，一些研究人员要么将此上下文提供给G和D，要么实现时间相干损失，要么使用RNN，要么对其进行组合。





### 4 