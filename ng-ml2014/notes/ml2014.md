## 第一章 引言

### 1-1 欢迎

机器学习应用：

- Database mining
  - **large datasets** from growth of automation/web.
  - e.g., Web click data, medical records, biology, engineering

- Applications can't program by hand.
  - e.g., Autonomous helicopter, handwriting recognition, NLP, CV
- Self-customizing programs
  - e.g., Amazon product recommendations

- Understanding human learning (brain, real AI)



### 1-2 What is ML

经验E、任务T、度量P：**机器学习是 T, measured by P, improves with E.**

ML algorithms:

- Supervised learning（监督学习）
- Unsupervised learning（无监督学习）
- Others: Reinforcement learning（强化学习）, recommender systems（推荐系统）



### 1-3 监督学习

"right answers" given：给一个数据集，含有一部分正确答案。使用监督学习算法得到对应关系。

监督学习类型：

- regression（回归问题）：输出具体值。如估计房价
- classification（分类问题）：输出离散值。如判断肿瘤

Q：实际问题，有多个attributes和多个features，甚至无穷个，如何处理？

A：以支持向量机算法为例，有数学方法来处理无穷多的features。



### 1-4 无监督学习

给定数据集没有label，不知道有哪些类型。使用无监督学习算法寻找数据中的结构。

无监督学习类型：

- 使用clustering（聚类算法）来自动分簇。

  如：新闻分簇，DNA分组；组织计算机群、社交网络分析、市场分割、天文数据分析。

- Cocktail party problem，两个麦克录下两个人同时说话，将他们分离。





## 第二章 单变量线性回归

### 2-1 regression模型描述

训练集

- m =训练样本的个数
- x = input variable/features
- y = output variable/features
- (x, y) = 一个训练样本
- (x<sup>(i)</sup>, y<sup>(i)</sup>) = 第i个样本



机器学习算法从训练集得出函数h（hypothesis），表示从x到y的对应关系。

在预测房价问题上，假定h是一元一次函数。

<img src='C:\Users\asus\Desktop\ml\andrewng\1.jpg' align='left'/>




### 2-2 代价函数

现在有h函数：$h_\theta(x) = \theta_0 + \theta_1x$，还有一个训练集。

我们的目的是寻找h函数中的两个参数θ<sub>0</sub>、θ<sub>1</sub>，让h<sub>θ</sub>(x)拟合实际y.

定义代价函数，计算h和y的差，要让其取最小，表达式如下：
$$
minimize_{\theta_0, \theta_1}  \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2
$$
也就是使代价函数  $ J({\theta_0, \theta_1}) = \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2 $  取到最小值。



### 2-3 代价函数的直观理解I

- Hypothesis:		$h_\theta(x) = \theta_0 + \theta_1x$

- Parameters:		$\theta_0, \theta_1$

- Cost Funcion:	 $J({\theta_0, \theta_1}) = \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2$

- Goal:					$minimize_{\theta_0, \theta_1} J({\theta_0, \theta_1})$ 

取不同的θ<sub>0</sub>θ<sub>1</sub>，得到不同的h(x)，对应不同的J(θ<sub>0</sub>, θ<sub>1</sub>).

下图是只有一个变量的简化情况，此时可以画出一元函数 J(θ<sub>1</sub>) 的曲线。

<img src='C:\Users\asus\Desktop\ml\andrewng\2.jpg' align='left'/>

选择θ，使J的值最小化。此时h函数最好地拟合了现实情况。



### 2-4 代价函数的直观理解II

有两个参数时，得到 J(θ<sub>0</sub>, θ<sub>1</sub>) 的三维图像。

<img src='C:\Users\asus\Desktop\ml\andrewng\3.jpg' align='left' width="50%" height="50%"/>

可以用等高线图在平面上展示。中心点处函数值最小。

<img src='C:\Users\asus\Desktop\ml\andrewng\4.jpg' align='left' width="50%" height="50%"/>

我们希望有一个算法，来**自动找到使 J 最小的参数 θ** .



### 2-5 梯度下降

1. 开始时，给θ设置初始值；

2. 改变θ，使 J 的值减少，直到取到了局部最小值；

3. 重复上个过程。

直观解释：

<img src='C:\Users\asus\Desktop\ml\andrewng\5.jpg' align='left' width="50%" height="50%"/>

​		不同的初始点可能走到不同的结束点。

数学解释：

<img src='C:\Users\asus\Desktop\ml\andrewng\6.jpg' align='left' width="70%" height="70%"/>

​		α 是学习率，表示梯度下降的步幅大小。偏导数表示梯度下降的方向。

​		需要注意的是，要让多个 θ 同时更新。先计算多个temp值，再一起赋新值。



### 2-6  梯度下降的直观理解

偏导数的直观解释：

<img src='C:\Users\asus\Desktop\ml\andrewng\7.jpg' align='left' width="50%" height="50%"/>

​		以单个变量的函数为例。偏导数的值保s证了 θ 一定朝 J 下降的方向变化。

学习率 α 的直观解释：

<img src='C:\Users\asus\Desktop\ml\andrewng\8.jpg' align='left' width="50%" height="50%"/>

​		α 太小，步数太多；α 太大，会导致无法收敛甚至发散。

在到达optimum点后，偏导数值为0，梯度下降算法就什么也不做了。

当学习率 α 不变时，梯度下降进行的过程中，偏导数通常会变小，因此每一步下降幅度会减小。

因此在接近局部最小值时，步子会变小。

<img src='C:\Users\asus\Desktop\ml\andrewng\9.jpg' align='left' width="50%" height="50%"/>



### 2-7 线性回归的梯度下降

<img src='C:\Users\asus\Desktop\ml\andrewng\10.jpg' align='left' width="70%" height="70%"/>

<img src='C:\Users\asus\Desktop\ml\andrewng\11.jpg' align='left' width="70%" height="70%"/>

这个梯度下降算法称为 **batch** 梯度下降。

​		原因是：每一步梯度下降，都计算了**整个训练集m个样本**的插值平方总和。

​		也有方法不全览整个训练集，每次只关注小子集。这将在之后介绍。

接下来的课：

- 在线性代数上，存在一个解法，可以在不需要多步梯度下降的情况下，也能解出代价函数的最小值，这是另一种称为正规方程(**normal equations**)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。
- 梯度下降的通用算法





## 第三章 线性代数

### 3-1 矩阵和向量

向量是一个 n × 1 的矩阵。

默认的下标从1开始。



### 3-2 加法和标量乘法



### 3-3 矩阵与向量相乘

一元线性回归可以转换成矩阵和向量相乘。

下图是矩阵和向量的构造方法，以及代码。

<img src='C:\Users\asus\Desktop\ml\andrewng\12.jpg' align='left' width="70%" height="70%"/>



### 3-4 矩阵乘法

<img src='C:\Users\asus\Desktop\ml\andrewng\13.jpg' align='left'/>



### 3-5 矩阵乘法的性质

矩阵乘法是：

- 不可交换的 A × B ≠ B × A 
- 可结合的  (A × B)× C = A ×(B× C）

单位矩阵 I ：

- A · I = I · A = A



### 3-6 逆、转置

矩阵的逆

- AA<sup>-1</sup> = A<sub>-1</sub>A = I ，A是满秩方阵

矩阵的转置

- A<sup>T</sup><sub>ij</sub> = A<sub>ji</sub>





-----

## 第四章 多变量线性回归

### 4-1 多维特征

从只有1个变量的情况，推广到有m组n维特征的情况。

$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$

设x<sub>0</sub> = 1，则$h_\theta(x) = \theta^Tx$（写成向量内积表达式）



### 4.2 多变量梯度下降

<img src='C:\Users\asus\Desktop\ml\andrewng\14.jpg' align='left' width="70%" height="70%"/>





### 4.3 梯度下降技巧1-特征缩放

当多个特征取值范围相差很大，梯度下降收敛得很慢。

因此，进行**Feature Scaling**，将每个特征都控制在约 -1 ≤ x<sub>i</sub> ≤ 1 的范围内。

<img src='C:\Users\asus\Desktop\ml\andrewng\15.jpg' align='left' width="70%" height="70%"/>

 除了除以最大值，还有一个均值归一化的工作（**mean normalization**），让特征的均值接近0.

具体做法是用 （x<sub>i</sub> - μ<sub>i</sub>）代替 x<sub>i</sub>。

<img src='C:\Users\asus\Desktop\ml\andrewng\16.jpg' align='left' width="70%" height="70%"/>



### 4.4 梯度下降技巧2-学习率

绘制随迭代次数增加，代价函数值的变化图象，来确定梯度下降算法在正常运行。

也可以用设置阈值（检测平滑）的方式自动测试，但确定阈值是困难的。看图像在大多数时候更方便直观。

<img src='C:\Users\asus\Desktop\ml\andrewng\17.jpg' align='left'/>

当代价函数曲线是上升的或不收敛，通常的解决方法是使用更小的 α 值。

α 太小会让收敛变得很慢，多试几次，选一个合适的 α 值。



### 4.5 选择特征和多项式回归

通过对函数图像的了解，和对数据的了解，**选择合适的特征**，来获得更好的模型。

如预测房屋价格，可以选择房屋的size，或者房屋的宽度等特征。

用多项式回归（**polynomial regression**）来理解选择特征：

<img src='C:\Users\asus\Desktop\ml\andrewng\18.jpg' align='left'/>

​		作简单的处理来拟合多项式模型：让x<sub>i</sub>为size的i次方，或对size开方。

​		在这种指数变换的情况下，做特征scaling是很有必要的。

也有些算法能够观察给出的数据，**自动**选择特征。



### 4.6 正规方程

对于某些**线性回归问题**，正规方程可以求解参数 θ 的最优值。

**正规方程：**

当θ是一个实数，让代价函数导数为0，可以解出θ的值。

推广到θ是向量，通过设置代价函数的偏导数为0，求解θ。

<img src='C:\Users\asus\Desktop\ml\andrewng\19.jpg' align='left' width="70%" height="70%"/>

具体做法：构造 m*(n+1) 矩阵 X 和 m 维向量 y ，用最小二乘法计算 θ 。

<img src='C:\Users\asus\Desktop\ml\andrewng\20.jpg' align='left' width="70%" height="70%"/>



<img src='C:\Users\asus\Desktop\ml\andrewng\21.jpg' align='left' width="70%" height="70%"/>

使用正规方程法，不需要做特征scaling。



线性回归问题的求解参数 θ：

- 梯度下降：在减小代价函数的过程中，**迭代变换  θ ** 。
  - 需要选择 α 并进行需要多次迭代

- 正规方程：**解析求解，只需一步**。
  - n 很大时，(X<sup>T</sup>X)<sup>-1</sup> 很难算（n10000时是进行选择的边缘）



### 4.7 正规方程及不可逆性（选修）

使用正规方程求解参数 $\theta = (X^TX)^{-1}X^Ty$ 时，如果：

**X<sup>T</sup>X不可逆**时（其实发生得很少）

- 咋办？	如在Octave里，有**求伪逆的函数 pinv**，矩阵不可逆时也可以正常求解。 
- 原因？    矩阵不满秩。
  - **多个 x 线性相关**。修改冗余的特征
  - **m≤n（特征多，数据少）**。用正则化（**regularization**）解决

