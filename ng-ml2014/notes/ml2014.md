- 第五章numpy&pandas形成文章后链接到博客
- 所有笔记完成后，将图片上传至MyPostImg，并修改文章中本地图片链接为远程（本地版本不用改，博文版本改一下）
- 考虑是否 align='left' （.jpg' --> .jpg' align='left')
- 不同的机器：
  - G:\github-repos
  - C:\Users\acbgzm\Documents\GitHub

## 第一章 引言

### 1-1 欢迎

机器学习应用：

- Database mining
  - **large datasets** from growth of automation/web.
  - e.g., Web click data, medical records, biology, engineering

- Applications can't program by hand.
  - e.g., Autonomous helicopter, handwriting recognition, NLP, CV
- Self-customizing programs
  - e.g., Amazon product recommendations

- Understanding human learning (brain, real AI)



### 1-2 What is ML

经验E、任务T、度量P：**机器学习是 T, measured by P, improves with E.**

ML algorithms:

- Supervised learning（监督学习）
- Unsupervised learning（无监督学习）
- Others: Reinforcement learning（强化学习）, recommender systems（推荐系统）



### 1-3 监督学习

"right answers" given：给一个数据集，含有一部分正确答案。使用监督学习算法得到对应关系。

监督学习类型：

- regression（回归问题）：输出具体值。如估计房价
- classification（分类问题）：输出离散值。如判断肿瘤

Q：实际问题，有多个attributes和多个features，甚至无穷个，如何处理？

A：以支持向量机算法为例，有数学方法来处理无穷多的features。



### 1-4 无监督学习

给定数据集没有label，不知道有哪些类型。使用无监督学习算法寻找数据中的结构。

无监督学习类型：

- 使用clustering（聚类算法）来自动分簇。

  如：新闻分簇，DNA分组；组织计算机群、社交网络分析、市场分割、天文数据分析。

- Cocktail party problem，两个麦克录下两个人同时说话，将他们分离。





## 第二章 单变量线性回归（Linear Regression with One Variable）

### 2-1 regression模型描述

训练集

- m =训练样本的个数
- x = input variable/features
- y = output variable/features
- (x, y) = 一个训练样本
- (x<sup>(i)</sup>, y<sup>(i)</sup>) = 第i个样本



机器学习算法从训练集得出函数h（hypothesis），表示从x到y的对应关系。

在预测房价问题上，假定h是一元一次函数。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\1.jpg' />




### 2-2 代价函数（cost function）

现在有h函数：$h_\theta(x) = \theta_0 + \theta_1x$，还有一个训练集。

我们的目的是寻找h函数中的两个参数θ<sub>0</sub>、θ<sub>1</sub>，让h<sub>θ</sub>(x)拟合实际y.

定义代价函数，计算h和y的差，要让其取最小，表达式如下：
$$
minimize_{\theta_0, \theta_1}  \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2
$$
也就是使代价函数  $ J({\theta_0, \theta_1}) = \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2 $  取到最小值。



### 2-3 代价函数的直观理解I

- Hypothesis:		$h_\theta(x) = \theta_0 + \theta_1x$

- Parameters:		$\theta_0, \theta_1$

- Cost Funcion:	 $J({\theta_0, \theta_1}) = \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2$

- Goal:					$minimize_{\theta_0, \theta_1} J({\theta_0, \theta_1})$ 

取不同的θ<sub>0</sub>θ<sub>1</sub>，得到不同的h(x)，对应不同的J(θ<sub>0</sub>, θ<sub>1</sub>).

下图是只有一个变量的简化情况，此时可以画出一元函数 J(θ<sub>1</sub>) 的曲线。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\2.jpg' />

选择θ，使J的值最小化。此时h函数最好地拟合了现实情况。



### 2-4 代价函数的直观理解II

有两个参数时，得到 J(θ<sub>0</sub>, θ<sub>1</sub>) 的三维图像。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\3.jpg'  width="50%" height="50%"/>

可以用等高线图在平面上展示。中心点处函数值最小。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\4.jpg'  width="50%" height="50%"/>

我们希望有一个算法，来**自动找到使 J 最小的参数 θ** .



### 2-5 梯度下降（gradient descent）

1. 开始时，给θ设置初始值；

2. 改变θ，使 J 的值减少，直到取到了局部最小值；

3. 重复上个过程。

直观解释：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\5.jpg'  width="50%" height="50%"/>

​		不同的初始点可能走到不同的结束点。

数学解释：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\6.jpg'  width="70%" height="70%"/>

​		α 是学习率，表示梯度下降的步幅大小。偏导数表示梯度下降的方向。

​		需要注意的是，要让多个 θ 同时更新。先计算多个temp值，再一起赋新值。



### 2-6  梯度下降的直观理解

偏导数的直观解释：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\7.jpg'  width="50%" height="50%"/>

​		以单个变量的函数为例。偏导数的值保s证了 θ 一定朝 J 下降的方向变化。

学习率 α 的直观解释：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\8.jpg'  width="50%" height="50%"/>

​		α 太小，步数太多；α 太大，会导致无法收敛甚至发散。

在到达optimum点后，偏导数值为0，梯度下降算法就什么也不做了。

当学习率 α 不变时，梯度下降进行的过程中，偏导数通常会变小，因此每一步下降幅度会减小。

因此在接近局部最小值时，步子会变小。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\9.jpg'  width="50%" height="50%"/>



### 2-7 线性回归的梯度下降

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\10.jpg'  width="70%" height="70%"/>

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\11.jpg'  width="70%" height="70%"/>

这个梯度下降算法称为 **batch** 梯度下降。

​		原因是：每一步梯度下降，都计算了**整个训练集m个样本**的插值平方总和。

​		也有方法不全览整个训练集，每次只关注小子集。这将在之后介绍。

接下来的课：

- 在线性代数上，存在一个解法，可以在不需要多步梯度下降的情况下，也能解出代价函数的最小值，这是另一种称为正规方程(**normal equations**)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。
- 梯度下降的通用算法





## 第三章 线性代数

### 3-1 矩阵和向量

向量是一个 n × 1 的矩阵。

默认的下标从1开始。



### 3-2 加法和标量乘法



### 3-3 矩阵与向量相乘

一元线性回归可以转换成矩阵和向量相乘。

下图是矩阵和向量的构造方法，以及代码。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\12.jpg'  width="70%" height="70%"/>



### 3-4 矩阵乘法

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\13.jpg' />



### 3-5 矩阵乘法的性质

矩阵乘法是：

- 不可交换的 A × B ≠ B × A 
- 可结合的  (A × B)× C = A ×(B× C）

单位矩阵 I ：

- A · I = I · A = A



### 3-6 逆、转置

矩阵的逆

- AA<sup>-1</sup> = A<sub>-1</sub>A = I ，A是满秩方阵

矩阵的转置

- A<sup>T</sup><sub>ij</sub> = A<sub>ji</sub>





-----

## 第四章 多变量线性回归（Linear Regression with Multiple Variables）

### 4-1 多维特征

从只有1个变量的情况，推广到有m组n维特征的情况。

$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$

设x<sub>0</sub> = 1，则$h_\theta(x) = \theta^Tx$（写成向量内积表达式）



### 4.2 多变量梯度下降

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\14.jpg'  width="70%" height="70%"/>



### <span id="4.3">4.3 梯度下降技巧1-特征缩放</span>

当多个特征取值范围相差很大，梯度下降收敛得很慢。

因此，进行**Feature Scaling**，将每个特征都控制在约 -1 ≤ x<sub>i</sub> ≤ 1 的范围内。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\15.jpg'  width="70%" height="70%"/>

 除了除以最大值，还有一个均值归一化的工作（**mean normalization**），让特征的均值接近0.

具体做法是用 （x<sub>i</sub> - μ<sub>i</sub>）代替 x<sub>i</sub>。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\16.jpg'  width="70%" height="70%"/>



### <span id="4.4">4.4 梯度下降技巧2-学习率</span>

绘制随迭代次数增加，代价函数值的变化图象，来确定梯度下降算法在正常运行。

也可以用设置阈值（检测平滑）的方式自动测试，但确定阈值是困难的。看图像在大多数时候更方便直观。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\17.jpg' />

当代价函数曲线是上升的或不收敛，通常的解决方法是使用更小的 α 值。

α 太小会让收敛变得很慢，多试几次，选一个合适的 α 值。



### <span id="4.5">4.5 选择合适的特征和多项式回归（polynomial regression）</span>

通过对函数图像的了解，和对数据的了解，**选择合适的特征**，来获得更好的模型。

如预测房屋价格，可以选择房屋的size，或者房屋的宽度等特征。

用多项式回归（**polynomial regression**）来理解选择特征：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\18.jpg' />

​		作简单的处理来拟合多项式模型：让x<sub>i</sub>为size的i次方，或对size开方。

​		在这种指数变换的情况下，做特征scaling是很有必要的。

也有些算法能够观察给出的数据，**自动**选择特征。



### 4.6 正规方程

对于某些**线性回归问题**，正规方程可以求解参数 θ 的最优值。

**正规方程：**

当 θ 是一个实数，让代价函数导数为0，可以解出 θ 的值。（函数求极小值，找导数为 0 的点）

推广到 θ 是向量，通过设置代价函数的偏导数为0，求解 θ 。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\19.jpg'  width="70%" height="70%"/>

具体做法：构造 m*(n+1) 矩阵 X 和 m 维向量 y ，用最小二乘法计算 θ 。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\20.jpg'  width="70%" height="70%"/>



<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\21.jpg'  width="70%" height="70%"/>

**使用正规方程法，不需要做特征scaling。**



线性回归问题的求解参数 θ：

- 梯度下降：在减小代价函数的过程中，**迭代变换  θ ** 。
  - 需要选择 α 并进行需要多次迭代

- 正规方程：**解析求解，只需一步**。
  - n 很大时，(X<sup>T</sup>X)<sup>-1</sup> 很难算（n10000时是进行选择的边缘）



### <span id="4.7">4.7 *正规方程及不可逆性</span>

使用正规方程求解参数 $\theta = (X^TX)^{-1}X^Ty$ 时，如果：

**X<sup>T</sup>X不可逆**时（其实发生得很少）

- 咋办？	如在Octave里，有**求伪逆的函数 pinv**，矩阵不可逆时也可以正常求解。 
- 原因？    矩阵不满秩。
  - **多个 x 线性相关**。修改冗余的特征
  - **m≤n（特征多，数据少）**。用正则化（**regularization**）解决





-----

## 第五章 Octave Tutorial

由于Octave不再具有先进性，我学习了Python3的numpy和pandas库，[笔记见代码和注释](https://github.com/ACBGZM/ml-notes/tree/master/ng-ml2014/code/01-numpyandpandas)。

***（todo：此处总结成一篇文章后，链接到个人博客去）***

### 5.6 向量化

将一般的运算转化成**使用线性代数库的矩阵、向量运算**。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\22.jpg'  width="70%" height="70%"/>

在线性回归问题，同步更新θ的问题上，向量运算如下：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\23.jpg' />

- **θ 是向量，α 是数，δ 是向量**。最终目的是**更新 θ 向量**。
- 对于 δ ，是由多个数组成的向量。
  - δ<sub>k</sub> 由 h-y和x<sub>k</sub><sup>(i)</sup>相乘再相加而来，**h-y是插值是一个数，x<sub>k</sub><sup>(i)</sup>是第i行第k个属性值也是一个数，δ<sub>k</sub> 就是一个数**。
  - **δ 是由数组成的向量**。
  - 整体上，也可以看做图片上 $u = 2v + 5w$ 的向量相加形式：先纵向形成向量，**所有的h-y是一样的，也就是是一个倍数，x是第i行数据的一个向量。δ就像u一样，做类似的向量×系数再相加**。





-----

## 第六章 逻辑回归（Logistic Regression）

当要预测的 y 是离散的，就是 Classification 问题。

Logistic Regression 算法就是一个广泛应用的 Classification 算法。

### 6.1 分类问题（classification）

Q：为什么线性回归不好用了？

A：线性回归解决这类问题的方法是拟合后设置阈值，再区分成离散值。

​	   如图，当在远处值，拉低了直线的斜率，就会让前面的肿瘤被误判成0.

（别忘了 x<sub>0</sub> 默认为1，以此让 θ<sub>0 </sub>作为偏移量，让直线离开原点）

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\24.jpg'  />

另外，与linear regression不同，logistic regression算法需要让 $h_\theta(x)$ 的值在 [0, 1]。



### 6.2 假设表示（hypothesis representation）

当有一个分类问题，我们要用哪个方程，来表示我们的假设？

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\25.jpg'  />

Sigmoid/Logistic 方程如图，我们通过拟合出 θ，让 x 和 h 反应真实情况。

从结果上，我们的结论是 $h_\theta(x)=P(y=1|x;\theta)$ ，即：x 这个样本，有 h 的概率，是 y=1 代表的情况.

（" probability that y = 1, given x, parameterized by θ "）

 

### 6.3 判定边界（decision boundary）

这个概念让我们理解假设函数 h 是如何做出预测的。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\26.jpg'  width="60%" height="60%"/>

- 当 $\theta^Tx ≥ 0$ 时，有  $h_\theta(x) = g(\theta^Tx) ≥ 0.5$ ，取离散值 y = 1；

- 当 $\theta^Tx ≤ 0$ 时，有  $h_\theta(x) = g(\theta^Tx) ≤ 0.5$ ，取离散值 y = 0。

一个例子：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\27.jpg'  width="60%" height="60%"/>

以途中两个x的情况为例，$\theta$ 可以确定一条直线，把 y = 1 和 y = 0 的情况分隔开，这条直线就叫判定边界。

在 [4.5 节](#4.5)中介绍了多项式回归，在特征 x 中，添加额外的高阶多项式项。在逻辑回归中也适用：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\28.jpg'  width="60%" height="60%"/>

区别是线性回归改的是 $h_\theta(x) = \theta^Tx$，而逻辑回归问题改的是 $\theta^Tx$ ， $h_\theta(x) = g(\theta^Tx)$。

需要注意的是：判定边界是由 θ 确定的，不是由训练集定义的。训练集的 x 所做的只是拟合出合适的 θ。

问题来了：如何根据数据，自动拟合出参数 θ ？（有些包的函数可以，如scipy）



### 6.4 代价函数（cost function）

问题如下：如何选择 θ ？该定义怎样的代价函数来迭代 θ ？

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\29.jpg'  width="50%" height="50%"/>

在线性回归问题中，$h_\theta(x)=\theta^Tx$ 是线性函数，定义 $Cost(h_\theta(x), y) = \frac{1}{2}(h_\theta(x)-y)^2$ ，进而定义代价函数 $J(\theta) = \frac{1}{m} \Sigma Cost$。$J(\theta)$是 convex 函数，梯度下降可以应用。

而在逻辑回归问题中， $h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$ **不是线性**的，如果还是像线性回归那样计算 $J(\theta)$ ，会发现 $J(\theta)$ 是一个 **non-convex 函数，使梯度下降无法应用**。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\30.jpg'  width="60%" height="60%"/>

需要另外找一个是凸函数的Cost函数：
$$
\begin{equation}
Cost(h_\theta(x), y)=\left\{
\begin{array}{rcl}
-log(h_\theta(x)) & \text{if} \quad y=1\\
-log(1-h_\theta(x)) & \text{if} \quad y=0\\
\end{array} \right.
\end{equation}
$$

- y = 1 时，预测出的 h 越偏离 1，Cost 越大；

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\31.jpg'  width="60%" height="60%"/>

- y = 0 时，预测出的 h 越偏离 0，Cost 越大。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\32.jpg'  width="60%" height="60%"/>

本节定义了单训练样本的代价函数，虽然没有进行详细的凸性分析，但代价函数此时是凸函数。接下来将扩展此结论，给出整个训练集的代价函数的定义。并给出更简单的写法。



### 6.5 简化代价函数和应用梯度下降

**简化代价函数：**

- 逻辑回归的代价函数：

$$
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost({h_\theta(x^{(i)}), y^{(i)}})
$$

$$
\begin{equation}
Cost(h_\theta(x), y)=\left\{
\begin{array}{rcl}
-log(h_\theta(x)) & \text{if} \quad y=1\\
-log(1-h_\theta(x)) & \text{if} \quad y=0\\
\end{array} \right.
\end{equation}
$$

$$
Note:y总是取0或1
$$

- Cost函数简写为：

$$
Cost(h_\theta(x), y)= -y \log h_\theta(x)-(1-y) \log(1-h_\theta(x))
$$

- 代价函数简写为：

$$
J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}y^{(i)} \log h_\theta(x^{(i)})+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]
$$

Q：为什么选择这样形式的代价函数？

A：虽然没有详细解释，但这个式子是通过极大似然法得来的，是统计学中，为不同的模型快速寻找参数的方法。并且这是一个convex函数。

**应用梯度下降：**

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\33.jpg' />

（todo：↑ 上式少写了一个 $\frac{1}{m}$？）

从形式上，逻辑回归和线性回归的梯度下降表达式一样。但两者的 $h_\theta(x)$ 定义不同，实际上是完全不同的东西。

在线性回归介绍的方法，像 [如何监控梯度下降正常运行](#4.4)、[feature scaling](#4.3) 在逻辑回归也适用。



### 6.6 高级优化

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\34.jpg' />

除了梯度下降，还有其他的高级算法能让 $J(\theta)$ 收敛。如共轭梯度、BFGS、L-BFGS。

只需知道用法即可，不一定要弄清所有的实现细节，也不要自己去实现这些算法。

```matlab
function [jVal, gradient] = costFunction(theta)
	jVal = (theta(1) - 5) ^ 2 + (theta(2) - 5) ^ 2;
	gradient = zeros(2, 1);
	gradient(1) = 2 * (theta(1) - 5);
	gradient(2) = 2 * (theta(1) - 5);

options = optimset('GradObj', 'on', 'MaxIter', '100');
initialTheta = zeros(2, 1)
[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options)
```

以上代码调用 fminunc 来进行函数的收敛和 optTheta 的迭代。

对于一般场景的逻辑回归，多个theta的处理方式如下：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\35.jpg'  width="60%" height="60%"/>



### 6.7 多分类问题：一对多（multi-class classification）

训练集的数据有多个分类，如何拟合分类器？

one-vs-all 方法：把训练集的每个类别分别单独拟合一个分类器。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\36.jpg'  width="70%" height="70%"/>

训练一个逻辑回归分类器 $h_\theta^{(i)}(x)$ ，对于每个类别 $i$ ，分类器预测 $y = i$ 的概率。

为了预测任意一个输入 $x$ 的分类，取 $\substack{\max\\i}h_\theta^{(i)}(x)$ ，即让分类器取最大概率值的 $i$ 类别。





-----

## 第七章 正则化（Regularization）

### <span id="7.1">7.1 过拟合问题（overfitting）</span>

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\37.jpg'  width="70%" height="70%"/>

- **过拟合（overfit）**：过于贴近训练数据的特征了，在训练集上近乎完美的预测/区分了所有的数据，但是**缺乏泛化能力**，在新的测试集上表现不佳。

- **欠拟合（underfit）**：测试样本的特性没有学习到，或模型过于简单无法拟合或区分样本。



(**识别过拟合、欠拟合的情况。**)

当 **features 的数量太多**，甚至比 training data 的数量都多，就很有可能出现过拟合。比如像窗户的数量、门的数量等特征，看上去都与房屋的价格有关。

**解决过拟合问题的方法**：

- **减少特征的数量**
  - 人工选择特征，进行取舍。舍弃特征的同时也会舍弃信息。
  - 使用模型选择算法，自动选择保留的特征。（在之后介绍）
- **正则化（regularization）**
  - 保留所有特征，但减少参数 $\theta_j$ 的量级/大小。
  - 情况：有多个特征、并且每个特征都是或多或少有用的，我们不希望把他们舍弃掉。



### <span id="7.2">7.2 正则化的直观理解、代价函数（cost function）</span>

#### 正则化的直观理解：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\38.jpg'  width="60%" height="60%"/>

通过减小参数 θ<sub>3</sub>、θ<sub>4</sub> 的值，给 x<sub>3</sub>、x<sub>4</sub> 两个特征加入惩罚，曲线就跟二次函数没什么区别了。

这就是正则化背后思想：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\39.jpg'  width="60%" height="60%"/>

不知道具体哪些特征 x 该舍弃掉，就**减小所有参数 θ** 。

#### 具体方法

在 $J(\theta)$ 后面加上一项 $\lambda\sum^{n}_{i=1}\theta_j^2$ ，在收敛 $J(\theta)$ 的同时让所有 $\theta$ 变小。

tips：

- 通常不含常数项 $\theta_0$ ，但影响不大。
- 要**选择合适的正则化参数 $\lambda$**。$\lambda$ 的作用是在两个目标中平衡：表达式前面的项代表**对数据更好的拟合**，后面的项代表**让参数尽量小来避免过拟合**。
  - $\lambda$ 越大，$\theta$ 的惩罚越大。如果 $\lambda$ 太大，$\theta$ 都接近0，就相当于把假设函数的项都忽略掉了，最后只剩一个常数项 $\theta _0$，造成欠拟合。

(问题：**如何自动选择 $\lambda$** ？)



### <span id="7.3">7.3 线性回归的正则化（regularized linear regression）</span>

拟合线性回归模型的两种算法：**基于梯度下降、基于正规方程**。

#### 梯度下降

在 $J(\theta)$ 的最后加一项 $\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$ 。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\40.jpg'  width="60%" height="60%"/>

梯度下降，求偏导 $\frac{\lambda}{m}\theta_j$ ：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\41.jpg'  width="60%" height="60%"/>

直观的理解，正则化的梯度下降是让 $\theta_j$ **每次额外乘以一个比1略小的数** $(1-\alpha\frac{\lambda}{m})$ ，每次都把参数改小一点。

将 θ<sub>0</sub> 和其余 θ 区别开。（$(1-\alpha\frac{\lambda}{m})$ 跟1差不多大，所以区分开的影响不大。可以对比两个式子看正则化给梯度下降带来的变化。）

#### 正规方程

回忆一下正规方程：构造 m*(n+1) 矩阵 X 和 m 维向量 y ，设定代价函数的导数为 0，用最小二乘法计算 θ。

正规方程正则化的方式是加一个 (n+1)*(n+1) 的方阵，这个方阵的构造如下图所示。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\42.jpg'  width="60%" height="60%"/>

***正则化中的 $X^TX$不可逆问题（见[4.7](#4.7)）**：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\43.jpg'  width="60%" height="60%"/>

在不进行正则化时，一些语言的库函数也可以**计算不可逆的矩阵**。

进行正则化后，如果 $\lambda>0$ ，可证得 $(X^TX+\lambda R)$ **一定是可逆的**。



### <span id="7.4">7.4 逻辑回归的正则化（regularized logistic regression）</span>

跟线性回归的梯度下降差不多。

在 $J(\theta)$ 的最后加一项 $\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$ 。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\44.jpg'  width="60%" height="60%"/>

梯度下降求偏导 $\frac{\lambda}{m}\theta_j$ ：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\45.jpg'  width="60%" height="60%"/>

实现方法：先定义代价函数：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\46.jpg'  width="60%" height="60%"/>

再把代价函数用到fminunc：```fminunc(@costFunction);``` 。





-----

## 第八章 神经网络：表述（Neural Networks: Representation）

### <span id="8.1">8.1 非线性假设</span>

Q：为什么要学习神经网络算法？（跟线性回归、逻辑回归相比有什么先进性）

A：如果**n太大，feature太多**，之前介绍的算法就不理想了。 对于许多实际的机器学习问题，n一般是很大的。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\47.jpg'  width="60%" height="60%"/>

如图，100个n会产生约5000个二次项，对应同样多的参数。如果包含所有的二次项，运算量会很大，并且很可能会出现过拟合的现象。如果再包含三次项，……。当**n很大**，通过增加feature来建立非线性分类器不是一个好办法。

而现实中的问题往往有很大的n。如50\*50分辨率的灰度图像，要存储每个像素的值，n=50\*50=2500；进行特征映射，n=2500*2500/2≈3,000,000。



### <span id="8.2">8.2 神经元和大脑</span>

可以用单个算法来模拟大脑的学习算法吗？

本节课举例论证了：人的一些感官是相通的——可以把任何sensor接入大脑，然后大脑的学习算法就能找出学习数据的方法，并处理这些数据。

现在的问题：如何找出大脑的学习算法？



### <span id="8.3">8.3 模型表示Ⅰ</span>

当使用神经网络时，如何表示我们的假设或模型？大脑里的神经元是相互连接的，通过接收、处理、传递电信号的方式工作 。

通过以下方式表示单个的人工神经元：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\48.jpg'  width="80%" height="80%"/>

模型的表示：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\49.jpg'  width="80%" height="80%"/>

- 用 $a_i^{(j)}$ 表示第 $j$ 层的第 $i$ 个神经元。

- 用 $\Theta^{(j)}$ 表示从第 $j$ 层到 $j+1$  层的权重矩阵。

- 如果一个网络，在第 $j$ 层有 $s_j$ 个单元，在第 $j+1$ 层有 $s_{j+1}$ 个单元，那么 $\Theta^{(j)}$ 的维度是 $s_{j+1}×(s_j+1)$。是**从后往前**的形式。
  - 我的理解：矩阵的行是下一层的每个神经元的权重列表，列对应上一层的神经元。列要加一是**加上上一层的偏置**，也就是$x_0$。



### <span id="8.4">8.4 模型表示Ⅱ：向量化和前向传播</span>

*本章直观地理解向量化的方法，并明白为什么这是学习复杂的非线性假设函数的好方法。*

向前传播的向量化：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\50.jpg'  width="80%" height="80%"/>

- $z^{(j+1)} = \Theta^{(j)}a^{(j)}$，每层的 $z:(s_{j+1}×1)$ 是偏置矩阵 $\Theta:(s_{j+1}×(s_j+1))$ 和上层的输出 $a:((s_j+1)×1)$ 做矩阵运算得出的。

- $a^{(j)} = g(z^{(j)})$，每行的输出 $a$ 是对 $z$ 做 sigmoid 运算。
- 每一层都添加偏置  $a_0^{(j)}=1$

这种前向传播的方法也可以帮助我们了解神经网络的作用，和神经网络算法为什么能在学习非线性假设函数时有好的表现。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\51.jpg'  width="80%" height="80%"/>

神经网络在每一层都像是做一个逻辑回归，根据输入拟合一些权值；而且每层的输入都是前层根据自动拟合的权值，计算得到的输出，因此可以包含一些很复杂的特征。神经网络可以利用隐藏层计算更复杂的特征，并最终输出到输出层。

*在接下来的两章，讨论具体的例子，描述如何利用神经网络来计算输入的非线性假设函数。*



### <span id="8.5">8.5 举例直观理解Ⅰ</span>

使用神经网络计算 AND 门：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\52.jpg'  width="80%" height="80%"/>

计算 OR 门：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\53.jpg'  width="80%" height="80%"/>

只要设置恰当的权值，神经网络就能起到相应的作用。



### <span id="8.6">8.6 举例直观理解Ⅱ</span>

使用感知机来解决非线性问题：亦或。

使用有一层隐藏层的神经网络计算 NOR 函数：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\54.jpg'  width="80%" height="80%"/>

x<sub>1</sub> NOR x<sub>2</sub> = (x<sub>1</sub> AND x<sub>2</sub>)    OR    ( (NOT x<sub>1</sub>) AND (NOT x<sub>2</sub>) )

通过识别手写数字的视频展示了，神经网络可以学习相当复杂的函数，并且有一定的抗干扰能力：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\55.jpg'  width="60%" height="60%"/>



### <span id="8.7">8.7 神经网络的多元分类问题（multi-class clasification）</span>

上节的手写数字识别问题就是多分类问题，需要识别10种类型的数字。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\56.jpg'  width="80%" height="80%"/>

方法是设置多个输出层神经元，再把训练集 $(x^{(i)},y^{(i)})$ 、输出 $h_\Theta(x^{(i)})=y^{(i)}$ 的 $y$ 都变成向量的形式。途中输出分成四类，则 $y$ 是四维向量。

*本章讲述了如何构建模型来表示神经网络算法。在下一章，将学习如何构建训练集，如何让神经网络自动学习参数。*





## 第九章 神经网络的学习（Neural Networks: Learning）

讲一个学习算法，可以在给定数据集上，为神经网络拟合参数。

### <span id="9.1">9.1 代价函数（cost function）</span>

#### 神经网络的符号表示：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\57.jpg'  width="80%" height="80%"/>

- $L$：层数
- $s_l$：第 $l$ 层的神经元数（不含偏置神经元）

- $K$：输出神经元的个数。
  - 二元神经网络的 $s_l=K=1$，多元神经网络的 $s_l=K$。
  - $h_\Theta(x) ∈ \R^K$



#### 神经网络的代价函数：

逻辑回归的代价函数一般表达式如下：
$$
J(\theta) = -\frac{1}{m}\bigg[\sum_{i=1}^{m}y^{(i)} \log h_\theta(x^{(i)})+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))\bigg]+\frac{\lambda}{2m}\sum_{j=1}^n\theta^2_j
$$
对于神经网络，有
$$
h_\Theta(x)∈\R^K,(h_\Theta(x))_i=i^{th} output
$$

$$
J(\Theta) = -\frac{1}{m}\bigg[\sum_{i=1}^{m}\sum_{k=1}^{K}y^{(i)}_k \log (h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k) \log(1-h_\Theta(x^{(i)}))_k\bigg] \\
+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta^{(l)}_{ji})^2
$$

与逻辑回归的代价函数不同点在于：

- 把 K 个输出神经元的损失加起来，再求 m 个数据上的平均

- 正则化项，取所有边权重的平方和。i 从1开始，不含偏移神经元的权重。



### <span id="9.2">9.2 反向传播算法（backpropagation algorithm）</span>

*一个让上节的损失函数取到最小值的算法*

上节定义了 $J(\Theta)$，如果想让 $J(\Theta)$ 取到最小值，就需要计算 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$

首先来看**前向传播的向量化**：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\58.jpg'  width="80%" height="80%"/>

此处可以仔细理解一下**前向传播的计算过程**。

- 以上图为例，如果只有一组数据集 $(x, y)$，
-  $a^{(1)}$的规格是 3×1，$\Theta^{(1)}$的规格是 5×3，
- 向量化后相乘，得到 $z^{(2)}$的规格是 5×1，求sigmoid后加一个偏移项则 $a^{(2)}$的规格是 6×1，
- $\Theta^{(2)}$的规格是 5×6，相乘得到 $z^{(2)}$的规格是 5×1，
- 以此类推。使用前向传播，可以从输入，通过神经网络，得到输出。



回到主题，为了计算导数项 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$，需要使用**反向传播算法**：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\59.jpg'  width="80%" height="80%"/>

- 对于每一个节点，我们计算这样一项 $\delta^{(l)}_j$，**代表了第 $l$ 层第 $j$ 个节点的“误差”**

- **从输出层，反向计算每层的误差**。

- 然后，根据 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta) = a^{(l)}_j\delta^{(l+1)}_i$（此式省略了 $\lambda$ 等参数），可以**求得想要的偏导数**。



#### 反向传播算法的整体过程

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\60.jpg'  width="80%" height="80%"/>

- 有训练集 $\{( x^{(1)},y^{(1)} ),...,(x^{(m)},y^{(m)})   \}$
- **设置初始误差矩阵** $\Delta^{(l)}_{ij}=0$
- For $i=1$ to $m$ ：
  - **设置输入层** $a^{(1)}=x^{(1)}$
  - 进行**前向传播**，求**每层输出** $a^{(l)}$
  - 使用 $y^{(i)}$ ，计算**输出层的误差**  $\delta^{(L)} =a^{(L)}-y^{(i)}$
  - 进行**反向传播**，计算**每层误差** $\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$
  - **累加误差矩阵** $\Delta^{(l)}_{ij}:=\Delta^{(l)}_{ij}+ a^{(l)}_j\delta^{(l+1)}_i$
- 计算 $D$
  - $D^{(l)}_{ij}:=\frac{1}{m}\Delta^{(l)}_{ij}+\lambda\Theta^{(l)}_{ij}$，if $j ≠0$
  - $D^{(l)}_{ij}:=\frac{1}{m}\Delta^{(l)}_{ij}$               ，if $j =0$

- $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta) = D^{(l)}_{ij}$ ，求得偏导
- 使用偏导进行梯度下降，或其他高级优化算法



补充：

- 反向传播不用计算 $\delta^{(1)}$ ，因为不需要对输入层考虑误差项。

- 累加误差矩阵写成向量相乘形式： $\Delta^{(l)}_{ij}:=\Delta^{(l)}_{ij}+ \delta^{(l+1)}_i(a^{(l)}_j)^T$



### <span id="9.3">9.3 反向传播算法的直观理解</span>

前向传播的直观理解：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\61.jpg'  width="80%" height="80%"/>

反向传播的直观理解：

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\62.jpg'  width="80%" height="80%"/>

可以把神经网络的代价函数中的主要部分，类比为线性回归的代价函数的方差计算。只需明确：本质上做的是“求与真实值y的偏离程度”这件事。

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\63.jpg'  width="90%" height="90%"/>

而反向传播过程中，计算的 $\delta_j^{(l)}$ 就是 **cost 关于 z 的偏导数**。具体来说，cost 是一个关于真实值 y 和神经网络的输出值 h(x) 的函数。 $\delta_j^{(l)}$实际上是 cost 关于这些计算出的中间项的偏导数。$\delta_j^{(l)}$衡量的是，为了影响这些**中间值 z **（进而影响**整个神经网络的输出 h**），我们想要改变的神经网络的**权重**的程度。



### <span id="9.4">9.4 使用注意：展开参数</span>

*怎样将参数从矩阵展开成向量，以满足高级最优化步骤中的使用需要*

<img src='C:\Users\acbgzm\Documents\GitHub\MyPostImage\ml-notes-img\andrewng\64.jpg'  width="90%" height="90%"/>

如上图，定义一个代价函数 costFunction，输入参数是 theta，函数返回代价值 jVal 以及导数值（梯度） gradient。

然后将这个函数传递给高级最优化算法 fminunc，这些库函数都假定 theta、initialTheta 是参数向量，同时假定代价函数的第二个返回值，也就是梯度值 gradient 也是一个向量。

这部分在我们使用逻辑回归的时候没有问题，但在神经网络中，参数矩阵 $\Theta$ 和梯度矩阵$D$ 都是矩阵而非向量，因此需要将这些矩阵展开成向量，从而作为参数输入到函数中。





















### <span id="9.5">9.5 梯度检验</span>







### <span id="9.6">9.6 随机初始化</span>







### <span id="9.7">9.7 组合起来</span>







### <span id="9.8">9.8 无人驾驶</span>
