- 第五章numpy&pandas形成文章后链接到博客
- 所有笔记完成后，将图片上传至MyPostImg，并修改文章中本地图片链接为远程（本地版本不用改，博文版本改一下）

## 第一章 引言

### 1-1 欢迎

机器学习应用：

- Database mining
  - **large datasets** from growth of automation/web.
  - e.g., Web click data, medical records, biology, engineering

- Applications can't program by hand.
  - e.g., Autonomous helicopter, handwriting recognition, NLP, CV
- Self-customizing programs
  - e.g., Amazon product recommendations

- Understanding human learning (brain, real AI)



### 1-2 What is ML

经验E、任务T、度量P：**机器学习是 T, measured by P, improves with E.**

ML algorithms:

- Supervised learning（监督学习）
- Unsupervised learning（无监督学习）
- Others: Reinforcement learning（强化学习）, recommender systems（推荐系统）



### 1-3 监督学习

"right answers" given：给一个数据集，含有一部分正确答案。使用监督学习算法得到对应关系。

监督学习类型：

- regression（回归问题）：输出具体值。如估计房价
- classification（分类问题）：输出离散值。如判断肿瘤

Q：实际问题，有多个attributes和多个features，甚至无穷个，如何处理？

A：以支持向量机算法为例，有数学方法来处理无穷多的features。



### 1-4 无监督学习

给定数据集没有label，不知道有哪些类型。使用无监督学习算法寻找数据中的结构。

无监督学习类型：

- 使用clustering（聚类算法）来自动分簇。

  如：新闻分簇，DNA分组；组织计算机群、社交网络分析、市场分割、天文数据分析。

- Cocktail party problem，两个麦克录下两个人同时说话，将他们分离。





## 第二章 单变量线性回归（Linear Regression with One Variable）

### 2-1 regression模型描述

训练集

- m =训练样本的个数
- x = input variable/features
- y = output variable/features
- (x, y) = 一个训练样本
- (x<sup>(i)</sup>, y<sup>(i)</sup>) = 第i个样本



机器学习算法从训练集得出函数h（hypothesis），表示从x到y的对应关系。

在预测房价问题上，假定h是一元一次函数。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\1.jpg' align='left'/>




### 2-2 代价函数（cost function）

现在有h函数：$h_\theta(x) = \theta_0 + \theta_1x$，还有一个训练集。

我们的目的是寻找h函数中的两个参数θ<sub>0</sub>、θ<sub>1</sub>，让h<sub>θ</sub>(x)拟合实际y.

定义代价函数，计算h和y的差，要让其取最小，表达式如下：
$$
minimize_{\theta_0, \theta_1}  \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2
$$
也就是使代价函数  $ J({\theta_0, \theta_1}) = \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2 $  取到最小值。



### 2-3 代价函数的直观理解I

- Hypothesis:		$h_\theta(x) = \theta_0 + \theta_1x$

- Parameters:		$\theta_0, \theta_1$

- Cost Funcion:	 $J({\theta_0, \theta_1}) = \frac{1}{2m}\sum_{i=1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2$

- Goal:					$minimize_{\theta_0, \theta_1} J({\theta_0, \theta_1})$ 

取不同的θ<sub>0</sub>θ<sub>1</sub>，得到不同的h(x)，对应不同的J(θ<sub>0</sub>, θ<sub>1</sub>).

下图是只有一个变量的简化情况，此时可以画出一元函数 J(θ<sub>1</sub>) 的曲线。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\2.jpg' align='left'/>

选择θ，使J的值最小化。此时h函数最好地拟合了现实情况。



### 2-4 代价函数的直观理解II

有两个参数时，得到 J(θ<sub>0</sub>, θ<sub>1</sub>) 的三维图像。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\3.jpg' align='left' width="50%" height="50%"/>

可以用等高线图在平面上展示。中心点处函数值最小。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\4.jpg' align='left' width="50%" height="50%"/>

我们希望有一个算法，来**自动找到使 J 最小的参数 θ** .



### 2-5 梯度下降（gradient descent）

1. 开始时，给θ设置初始值；

2. 改变θ，使 J 的值减少，直到取到了局部最小值；

3. 重复上个过程。

直观解释：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\5.jpg' align='left' width="50%" height="50%"/>

​		不同的初始点可能走到不同的结束点。

数学解释：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\6.jpg' align='left' width="70%" height="70%"/>

​		α 是学习率，表示梯度下降的步幅大小。偏导数表示梯度下降的方向。

​		需要注意的是，要让多个 θ 同时更新。先计算多个temp值，再一起赋新值。



### 2-6  梯度下降的直观理解

偏导数的直观解释：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\7.jpg' align='left' width="50%" height="50%"/>

​		以单个变量的函数为例。偏导数的值保s证了 θ 一定朝 J 下降的方向变化。

学习率 α 的直观解释：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\8.jpg' align='left' width="50%" height="50%"/>

​		α 太小，步数太多；α 太大，会导致无法收敛甚至发散。

在到达optimum点后，偏导数值为0，梯度下降算法就什么也不做了。

当学习率 α 不变时，梯度下降进行的过程中，偏导数通常会变小，因此每一步下降幅度会减小。

因此在接近局部最小值时，步子会变小。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\9.jpg' align='left' width="50%" height="50%"/>



### 2-7 线性回归的梯度下降

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\10.jpg' align='left' width="70%" height="70%"/>

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\11.jpg' align='left' width="70%" height="70%"/>

这个梯度下降算法称为 **batch** 梯度下降。

​		原因是：每一步梯度下降，都计算了**整个训练集m个样本**的插值平方总和。

​		也有方法不全览整个训练集，每次只关注小子集。这将在之后介绍。

接下来的课：

- 在线性代数上，存在一个解法，可以在不需要多步梯度下降的情况下，也能解出代价函数的最小值，这是另一种称为正规方程(**normal equations**)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。
- 梯度下降的通用算法





## 第三章 线性代数

### 3-1 矩阵和向量

向量是一个 n × 1 的矩阵。

默认的下标从1开始。



### 3-2 加法和标量乘法



### 3-3 矩阵与向量相乘

一元线性回归可以转换成矩阵和向量相乘。

下图是矩阵和向量的构造方法，以及代码。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\12.jpg' align='left' width="70%" height="70%"/>



### 3-4 矩阵乘法

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\13.jpg' align='left'/>



### 3-5 矩阵乘法的性质

矩阵乘法是：

- 不可交换的 A × B ≠ B × A 
- 可结合的  (A × B)× C = A ×(B× C）

单位矩阵 I ：

- A · I = I · A = A



### 3-6 逆、转置

矩阵的逆

- AA<sup>-1</sup> = A<sub>-1</sub>A = I ，A是满秩方阵

矩阵的转置

- A<sup>T</sup><sub>ij</sub> = A<sub>ji</sub>





-----

## 第四章 多变量线性回归（Linear Regression with Multiple Variables）

### 4-1 多维特征

从只有1个变量的情况，推广到有m组n维特征的情况。

$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$

设x<sub>0</sub> = 1，则$h_\theta(x) = \theta^Tx$（写成向量内积表达式）



### 4.2 多变量梯度下降

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\14.jpg' align='left' width="70%" height="70%"/>



### 4.3 梯度下降技巧1-特征缩放

当多个特征取值范围相差很大，梯度下降收敛得很慢。

因此，进行**Feature Scaling**，将每个特征都控制在约 -1 ≤ x<sub>i</sub> ≤ 1 的范围内。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\15.jpg' align='left' width="70%" height="70%"/>

 除了除以最大值，还有一个均值归一化的工作（**mean normalization**），让特征的均值接近0.

具体做法是用 （x<sub>i</sub> - μ<sub>i</sub>）代替 x<sub>i</sub>。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\16.jpg' align='left' width="70%" height="70%"/>



### 4.4 梯度下降技巧2-学习率

绘制随迭代次数增加，代价函数值的变化图象，来确定梯度下降算法在正常运行。

也可以用设置阈值（检测平滑）的方式自动测试，但确定阈值是困难的。看图像在大多数时候更方便直观。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\17.jpg' align='left'/>

当代价函数曲线是上升的或不收敛，通常的解决方法是使用更小的 α 值。

α 太小会让收敛变得很慢，多试几次，选一个合适的 α 值。



### <span id="4.5">4.5 选择合适的特征和多项式回归（polynomial regression）</span>

通过对函数图像的了解，和对数据的了解，**选择合适的特征**，来获得更好的模型。

如预测房屋价格，可以选择房屋的size，或者房屋的宽度等特征。

用多项式回归（**polynomial regression**）来理解选择特征：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\18.jpg' align='left'/>

​		作简单的处理来拟合多项式模型：让x<sub>i</sub>为size的i次方，或对size开方。

​		在这种指数变换的情况下，做特征scaling是很有必要的。

也有些算法能够观察给出的数据，**自动**选择特征。



### 4.6 正规方程

对于某些**线性回归问题**，正规方程可以求解参数 θ 的最优值。

**正规方程：**

当θ是一个实数，让代价函数导数为0，可以解出θ的值。

推广到θ是向量，通过设置代价函数的偏导数为0，求解θ。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\19.jpg' align='left' width="70%" height="70%"/>

具体做法：构造 m*(n+1) 矩阵 X 和 m 维向量 y ，用最小二乘法计算 θ 。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\20.jpg' align='left' width="70%" height="70%"/>



<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\21.jpg' align='left' width="70%" height="70%"/>

使用正规方程法，不需要做特征scaling。



线性回归问题的求解参数 θ：

- 梯度下降：在减小代价函数的过程中，**迭代变换  θ ** 。
  - 需要选择 α 并进行需要多次迭代

- 正规方程：**解析求解，只需一步**。
  - n 很大时，(X<sup>T</sup>X)<sup>-1</sup> 很难算（n10000时是进行选择的边缘）



### 4.7 正规方程及不可逆性（选修）

使用正规方程求解参数 $\theta = (X^TX)^{-1}X^Ty$ 时，如果：

**X<sup>T</sup>X不可逆**时（其实发生得很少）

- 咋办？	如在Octave里，有**求伪逆的函数 pinv**，矩阵不可逆时也可以正常求解。 
- 原因？    矩阵不满秩。
  - **多个 x 线性相关**。修改冗余的特征
  - **m≤n（特征多，数据少）**。用正则化（**regularization**）解决





-----

## 第五章 Octive Tutorial

由于Octive不再具有先进性，我学习了Python3的numpy和pandas库，[笔记见代码和注释](https://github.com/ACBGZM/ml-notes/tree/master/ng-ml2014/code/01-numpyandpandas)。

***（todo：此处总结成一篇文章后，链接到个人博客去）***

### 5.6 向量化

将一般的运算转化成**使用线性代数库的矩阵、向量运算**。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\22.jpg' align='left' width="70%" height="70%"/>

在线性回归问题，同步更新θ的问题上，向量运算如下：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\23.jpg' align='left'/>

- **θ 是向量，α 是数，δ 是向量**。最终目的是**更新 θ 向量**。
- 对于 δ ，是由多个数组成的向量。
  - δ<sub>k</sub> 由 h-y和x<sub>k</sub><sup>(i)</sup>相乘再相加而来，**h-y是插值是一个数，x<sub>k</sub><sup>(i)</sup>是第i行第k个属性值也是一个数，δ<sub>k</sub> 就是一个数**。
  - **δ 是由数组成的向量**。
  - 整体上，也可以看做图片上 $u = 2v + 5w$ 的向量相加形式：先纵向形成向量，**所有的h-y是一样的，也就是是一个倍数，x是第i行数据的一个向量。δ就像u一样，做类似的向量×系数再相加**。





-----

## 第六章 逻辑回归（Logistic Regression）

当要预测的 y 是离散的，就是 Classification 问题。

Logistic Regression 算法就是一个广泛应用的 Classification 算法。

### 6.1 分类问题（classification）

Q：为什么线性回归不好用了？

A：线性回归解决这类问题的方法是拟合后设置阈值，再区分成离散值。

​	   如图，当在远处值，拉低了直线的斜率，就会让前面的肿瘤被误判成0.

（别忘了 x<sub>0</sub> 默认为1，以此让 θ<sub>0 </sub>作为偏移量，让直线离开原点）

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\24.jpg' align='left' />

另外，与linear regression不同，logistic regression算法需要让 $h_\theta(x)$ 的值在 [0, 1]。



### 6.2 假设表示（hypothesis representation）

当有一个分类问题，我们要用哪个方程，来表示我们的假设？

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\25.jpg' align='left' />

Sigmoid/Logistic 方程如图，我们通过拟合出 θ，让 x 和 h 反应真实情况。

从结果上，我们的结论是 $h_\theta(x)=P(y=1|x;\theta)$ ，即：x 这个样本，有 h 的概率，是 y=1 代表的情况.

（" probability that y = 1, given x, parameterized by θ "）

 

### 6.3 判定边界（decision boundary）

这个概念让我们理解假设函数 h 是如何做出预测的。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\26.jpg' align='left' width="60%" height="60%"/>

- 当 $\theta^Tx ≥ 0$ 时，有  $h_\theta(x) = g(\theta^Tx) ≥ 0.5$ ，取离散值 y = 1；

- 当 $\theta^Tx ≤ 0$ 时，有  $h_\theta(x) = g(\theta^Tx) ≤ 0.5$ ，取离散值 y = 0。

一个例子：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\27.jpg' align='left' width="60%" height="60%"/>

以途中两个x的情况为例，$\theta$ 可以确定一条直线，把 y = 1 和 y = 0 的情况分隔开，这条直线就叫判定边界。

在 [4.5 节](#4.5)中介绍了多项式回归，在特征 x 中，添加额外的高阶多项式项。在逻辑回归中也适用：

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\28.jpg' align='left' width="60%" height="60%"/>

区别是线性回归改的是 $h_\theta(x) = \theta^Tx$，而逻辑回归问题改的是 $\theta^Tx$ ， $h_\theta(x) = g(\theta^Tx)$。

需要注意的是：判定边界是由 θ 确定的，不是由训练集定义的。训练集的 x 所做的只是拟合出合适的 θ。

问题来了：如何根据数据，自动拟合出参数 θ ？



### 6.4 代价函数（cost function）

问题如下：如何选择 θ ？

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\29.jpg' align='left' width="50%" height="50%"/>

在线性回归问题中，$h_\theta(x)=\theta^Tx$ 是线性函数，定义 $Cost(h_\theta(x), y) = \frac{1}{2}(h_\theta(x)-y)^2$ ，进而定义代价函数 $J(\theta) = \frac{1}{m} \Sigma Cost$。$J(\theta)$是 convex 函数，梯度下降可以应用。

而在逻辑回归问题中， $h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$ **不是线性**的，如果还是像线性回归那样计算 $J(\theta)$ ，会发现 $J(\theta)$ 是一个 **non-convex 函数，使梯度下降无法应用**。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\30.jpg' align='left' width="60%" height="60%"/>

需要另外找一个是凸函数的代价函数：
$$
\begin{equation}
Cost(h_\theta(x), y)=\left\{
\begin{array}{rcl}
-log(h_\theta(x)) & \text{if} \quad y=1\\
-log(1-h_\theta(x)) & \text{if} \quad y=0\\
\end{array} \right.
\end{equation}
$$

- y = 1 时，预测出的 h 越偏离 1，Cost 越大；

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\31.jpg' align='left' width="60%" height="60%"/>

- y = 0 时，预测出的 h 越偏离 0，Cost 越大。

<img src='G:\github-repos\MyPostImage\ml-notes-img\andrewng\32.jpg' align='left' width="60%" height="60%"/>

本节定义了单训练样本的代价函数，虽然没有进行详细的凸性分析，但代价函数此时是凸函数。接下来将扩展此结论，给出整个训练集的代价函数的定义。并给出更简单的写法。



### 6.5 分类问题













### 6.6 分类问题



### 6.7 分类问题

### 6.8 分类问题

### 6.9 分类问题