我们提出了一个新的大规模数据集和随附的机器学习模型，旨在提供对视觉内容、其情感效果以及对后者的语言解释的详细了解。 与计算机视觉中大多数现有的注释数据集不同，我们专注于视觉艺术作品触发的情感体验，并要求注释者指出他们对给定图像的主要情感，最重要的是，还应为其情感提供扎实的口头解释。 如我们要展示的，这为图像的**客观内容**和**情感影响**带来了丰富的信号集，与**抽象概念**（例如，“自由”或“爱”）或超出其含义的概念相关联直接可见，包括视觉比喻和隐喻，或对个人经历的主观参考。 我们专注于视觉艺术（例如绘画，艺术照），因为它是为引起观众情感反应而创建的图像的主要示例。
我们的数据集称为ArtEmis，其中包含来自WikiArt的81K作品中的43.9万个来自人类的情感归因和解释。 基于这些数据，我们训练并演示了一系列字幕系统，它们能够表达和解释视觉刺激产生的情绪。 值得注意的是，这些系统产生的字幕通常可以成功地反映出图像的语义和抽象内容，远远超出了在现有数据集上训练的系统范围。

### 1. Introduction

情绪是人类经验中最普遍的方面。 虽然情感本身并不是语言结构，但我们对情感的最强大和永久的访问是通过语言[45]。 在这项工作中，我们专注于大规模收集和分析语言，这些语言解释了通过观察视觉艺术品而产生的情感。
具体来说，我们试图更好地理解艺术品的视觉属性、艺术品可能产生的主观情感体验以及通过语言解释此类情感的方式之间的联系。 基于这些数据和最新的机器学习方法，我们还设计和测试了基于神经的说话者，旨在模仿人类对视觉艺术的情感反应并提供相关的解释。

**为什么是视觉艺术？** 我们专注于视觉艺术品有两个原因。

- 首先也是最重要的原因是：艺术创作的目的通常就是引起观众的情感反应。 用托尔斯泰（Leo Tolstoy）的话说，“艺术是一种人类活动，其中一个人有意识地将自己所经历的感受传递给其他人，而其他人也被这些感受所感染，并且也体验到了它们” [56]。 
- 其次，艺术品，特别是抽象艺术形式，常常无视简单的解释，而且可能没有单一的，易于识别的主题或标签。 因此，情感反应可能需要对图像内容及其对观看者的影响进行整合的更详细的分析。 这与大多数自然图像不同，
  - 大多数自然图像通常通过纯粹客观的基于内容的标记机制，基于它们所包含的对象或动作来标记[14、13]。 
  - 不同的是，在艺术方面，我们的目标是<u>启动更加细微的感知图像理解</u>，在下游，也可以将其应用于对普通图像的更丰富理解。

我们通过引入一个称为ArtEmis [Art Emotions]的大规模数据集来开始这项工作，该数据集将人类的情感与艺术作品联系起来，并包含以自然语言解释每种触发的情感背后的解释。

![](G:\github-repos\MyPostImage\dl-labimg\affectiveabstractimage\artemis1.png)

如图，ArtEmis的描述更偏向抽象、想象、情感，而现存CV数据集的描述偏向客观实体、眼见为实。

**ArtEmis的新颖性。** 我们的数据集是新颖的，因为它涉及计算机视觉中尚未充分探索的问题：基于视觉刺激的语言情感解释的形成。 具体来说，<u>ArtEmis不仅展示情绪，感觉，个人态度，而且还展示抽象概念，例如自由或爱，以各种复杂的视觉刺激为基础（请参阅第3.2节）</u>。 注释者通常会描述视觉属性，并将其与心理解释联系起来，例如“她的年轻面孔突显她的纯真”，突出显示对象的特殊性，例如“她的脖子太长，这看起来不自然”； 并包括对没有直接出现在图像中但可能与拍摄对象的经历有关的对象的想象或隐喻描述；  “它让我想起了祖母”或“它看起来像鲜血”（超过20％的语料库包含这样的比喻）。

**反应的主观性。** 与现有的字幕数据集不同，ArtEmis欢迎情感解释（以字幕的形式）具有的主观和个人角度。 甚至一个人也可能对同一张给定的艺术图像产生一系列不同的情感反应[42、51、10、52]，如图2所示，这个主观差异体现在在不同的注释人之间。 主观性和丰富的语义内容将ArtEmis与广泛使用的COCO数据集区分开[14]。 图1显示了来自ArtEmis和COCO数据集的不同图像，包括标题“鸟”一词，其中<u>ArtEmis的想象力和隐喻性很明显</u>（例如，“鸟给人希望”和“笼中生活”）。 有趣的是，尽管有这种现象，如我们稍后将要看到的（第3.2节），（1）注解者之间关于他们的主要情感反应经常达成实质性共识，并且（2）我们收集的解释通常是务实的，即，它们也包含对视觉的引用。 图像中存在的元素（请参阅第3.3节）。

![](G:\github-repos\MyPostImage\dl-labimg\affectiveabstractimage\artemis2.png)

**情感解释的难点。** 在神经科学界内部，关于人类情感是否是先天性的，是由神经活动模式产生的，还是后天学习而来的，一直存在争论[54、4、9]。 用语言产生情感解释可能会存在固有的困难。因此，对于注释者而言，以传统的图像字幕所没有的方式来，这项任务可能会面临挑战。 我们的方法得到了重要研究的支持，该研究主张语言在捕捉甚至帮助形成情感方面的核心作用[37，6]，包括Lisa Feldman Barrett的“建构情感理论” [6，7，5，8]。 然而，这场辩论表明，在各种标准指标下，将ArtEmis与其他字幕数据集进行比较时，需要谨慎行事。

**情感neural speakers。** 为了进一步展示ArtEmis数据集的潜力，我们使用在我们的数据集上训练的深度学习语言生成技术，尝试构建了许多neural speaker。 我们最好的speaker通常会得出很好的情感解释，对抽象的视艺术图像做出反应，即使在与人类竞争时，在图灵情感测验中的表现也相当不错。

总而言之，我们做出以下主要贡献：

- 我们发布ArtEmis数据集，这是对视觉艺术品的情感反应的大规模数据集，并带有对情感的语言解释（第3节）。
- 我们展示了与现有数据集相比，所收集的语料库如何包含显着更具情感性，抽象性以及比喻和明喻的话语（第3.1-3.2节）。
- 使用ArtEmis，我们开发了机器学习模型，用于从图像或文本预测占主导地位的情绪，以及可以产生合理的基础情绪解释的neural speaker（第4和第6节）。



### 2. Background and related work

**情绪分类。** 根据先前的研究[40，63，68，49]，我们在整个工作中采用了八个离散的情感状态的相同离散集合。 具体来说，我们认为：<u>愤怒，厌恶，恐惧和悲伤是消极情绪，娱乐，敬畏，满足和兴奋是积极情绪</u>。 四个负面情绪被认为是普遍的和基本的（由Ekman在[22]中提出），并已被证明可以很好地捕获国际情感图片系统中的离散情绪[11]。 四种积极的情绪是幸福的细粒度版本[21]。 我们注意到，虽然敬畏感可以与负面情绪相关联，但根据先前的研究（[42，49]），我们在分析中将敬畏感视为一种积极的情感。

**深度学习，情感和艺术。**  Computer Vision中大多数现有的作品都将情感视为图像分类问题，并构建了试图推论给定图像将引起的主要/主要情感的系统[40，63，68，33]。 在[24]中给出了一项有趣的工作，将绘画与对其历史和社会复杂性的文字描述联系起来。 同样，[30]的工作尝试使用语言风格转换为莎士比亚散文中的绘画加上标题。 最后，[60]的工作介绍了带有多个属性注释的艺术图像的大规模数据集。 与这些作品不同，我们专注于开发机器学习工具，以分析和生成艺术品所唤起的对情绪的解释。

**字幕模型和数据。** 有很多工作和相应的字幕数据集[65，31，55，34，41，48]专注于人类认知的不同方面。 例如，COCO-captions[14]涉及自然图像中常见对象的描述，Monroe等人的数据集[43]包括2D单色的判别性参考文献，Achlioptas等。  [1，2]收集3D对象等的辨别话语。相应地，在基于深网的字幕方法[39、41、57、67、44、66、44]上也存在大量内容。  [59，29]的开创性著作通过利用深度递归网络（LSTM [27]）以及其他经典思想（例如，使用教师强迫训练[61]）取得的进展，开辟了这条道路。 我们的神经说者基于这些“标准”技术，ArtEmis为基于图像的字幕添加了新的维度，以反映情感。

**情绪驱动的字幕。** 关于情绪（积极情绪与消极情绪）的字幕工作要少得多。  Radfordandcolleagues [50]发现在没有情感标签的情况下训练的递归语言模型中的一个单元会自动学习情感概念。 并通过固定该单元的符号来实现面向情感的操纵。 其他早期工作，例如SentiCap [47]和后续工作，例如[64]，提供了基于情感的显式监控，以使基于真实世界图像的情感风味语言生成成为可能。 这些研究侧重于仅引起两种情绪反应（正面和负面）的视觉线索，最重要的是，它们不产生解释情绪的语言。



### 3, ArtEmis dataset

ArtEmis数据集建立在可公开获取的WikiArt数据集的基础上，该数据集包含来自1,119位艺术家（于2015年下载）精心制作的81,446件艺术品，涵盖了可追溯到15世纪的艺术品，到21世纪的现代美术作品。 这些艺术品涵盖了27种艺术风格（抽象，巴洛克，立体主义，印象派等）和45种流派（城市景观，风景，肖像，静物等），构成了非常多样化的视觉刺激[53]。 在ArtEmis中，我们对WikiArt的所有艺术品进行了注释，要求每件艺术品至少5位注释者表达其主要的情感反应，并用话语解释其反应背后的原因。

![](G:\github-repos\MyPostImage\dl-labimg\affectiveabstractimage\artemis3.png)

具体来说，在观察艺术品后，首先要求注释者通过选择第2节中提到的八种情绪或第九种选择（其他称为“ something-else”）来表明它们的主要反应。 设置后一种选项是为了允许注释者表达未明确列出的情感，或解释为什么他们可能没有强烈的情感反应，例如为什么对所展示的艺术品无动于衷。 在所有情况下，第一步之后，都要求注释者在自由文本中提供详细的选择说明，其中包括对艺术品中视觉元素的特定引用。 有关收集的注释的示例，请参见图1,2，有关使用的界面的快速概述，请参见图3。

我们总共收集了439,121个解释性话语和情绪反应。 生成的语料库包含36,347个不同的词，其中包括对6,377个注释者的解释，这些注释者总共花费了10,220个小时来构建它。 这些注释者是通过Amazon的Mechanical Turk（AMT）服务招募的。 接下来，我们分析ArtEmis的主要特征，同时将感兴趣的读者指向补充材料[3]，以获取更多详细信息。

#### 3.1 Linguistic analysis（语言分析）

**丰富性和多样性。**  ArtEmis字幕的平均长度为15.8个单词，比表1所示的许多现有字幕数据集的字幕的平均长度长得多。在同一表中，我们还显示了根据平均数量分析ArtEmis的结果 名词，代词，形容词，动词和介词的集合。 与许多现有数据集相比，ArtEmis对每种类别的字幕出现率更高，这表明我们的注释在与艺术品及其所表达的情感相关的方面提供了丰富的自然语言用法。 当我们查看独特的形容词，这些形容词用于解释不同注释者对同一艺术品的反应（表2）。 换句话说，除了语言丰富之外，收集到的解释也非常多样化。

![](G:\github-repos\MyPostImage\dl-labimg\affectiveabstractimage\artemis4.png)

**情绪分析。** 除了丰富多样之外，ArtEmis还包含感性的语言。我们使用基于规则的情感分析器（VADER [28]）来证明这一点。 分析仪仅将16.5％的ArtEmis分配给中性情绪，而对于COCO-captions，它将分配77.4％。 图4（c）显示了这两个数据集的VADER感性估算价的直方图。 绝对情感值接近0表示中性情绪。更多细节见补充资料。

![](G:\github-repos\MyPostImage\dl-labimg\affectiveabstractimage\artemis5.png)

> 图：ArtEmis的关键属性和面向类型的分析。
>
> 顶行：沿（a）具体性，（b）主观性和（c）情感的轴比较ArtEmis和COCO字幕的直方图。  ArtEmis具有比COCO-captions更多的抽象性，主观性和情感化语言。 
>
> 底行：（d）显示在同一艺术流派（genre）的不同作品中，ArtEmis中引起的情绪分布的平均熵。  （e）和（f）显示了顶部行中使用的“主观性”和“情感化”指标的平均值。

#### 3.2 Emotion-centric analysis（情感为中心的分析）

在图5中，我们展示了用户在所有收集的注释中选择的9个选项的直方图。

我们指出，选择积极情绪要比消极情绪多得多，而选择“别的”选项的可能性为11.7％。 有趣的是，有61％的作品被同时标注了至少一种积极和一种消极的情感（如果我们将其他事物视为第三情感类别，则这一比例为79％）。尽管此结果凸显了艺术品可能引发的情绪反应方面的高度主观性，但我们也注意到注释者之间就引起的情绪存在着显着的共识。 也就是说，有45.6％（37,145）的画作在注释者中表现出相同的细颗粒感，占绝大多数。

![](G:\github-repos\MyPostImage\dl-labimg\affectiveabstractimage\artemis6.png)

**语言使用的特质。** 在这里，我们探讨了ArtEmis包含抽象与具体，主观与客观的语言的程度，并估计了注释者在其解释中使用明喻和隐喻的程度。 为了执行此分析，我们标记收集到的话语，并将它们与携带相关元数据的外部策划的词典进行比较。 为了衡量抽象性或具体性，我们使用了Brysbaert等人的词典 [12]，为40,000个单词词条提供了1到5的评分，反映了它们的具体性。 例如，香蕉和百吉饼是最大的混凝土/有形物体，得分为5，而爱和心理则非常抽象（得分分别为2.07和1.34）。  ArtEmis的随机词具有2.80的具体性，而COCO的随机词具有3.55（p值有效，请参见图4（a））。 换句话说，<u>ArtEmis平均包含对更多抽象概念的引用</u>。 在将ArtEmis与其他广泛使用的字幕数据集进行比较时，也可以使用此方法（请参见补充材料）。

接下来，为了衡量ArtEmis使用主观语言的程度，我们应用TextBlob [38]提供的基于规则的算法，该算法通过在[0,1]中提供标量值来估计句子的主观程度。 例如，“画得很红”被认为是最大程度的客观话语（得分1），而“画得好”则是最大程度的主观（得分0）。 我们在图4（b）中显示了这些估计的结果分布。 最后，我们整理了一个引理列表，建议使用明喻的可能性很高（例如，“像”，“看起来”，“提醒我”）。 这样的表达出现在我们的语料库的20.5％上，并且如稍后所示，这个列表也被我们的神经系统说话者成功采用。

#### 3.3 Maturity, reasonableness & specificity（成熟度，合理性和特异性）

我们还通过进行三个单独的用户研究来调查ArtEmis的独特方面。 具体而言，我们旨在了解：a）某人表达随机ArtEmis解释所需的情感和认知成熟度是什么？，b）听众找到随机ArtEmis解释的合理性，即使他们不愿意使用它来描述自己的反应？，最后，c）收集到的解释可在多大程度上用于区分一件艺术品和另一件艺术品？ 我们以二进制（yes / no）的形式向Turkers提出第一个问题，方法是向他们展示随机选择的艺术品及其随附的解释，并询问他们这种解释是否需要比非典型4岁孩子更高的情感成熟度。  1K语音的答案是“是”的时间为76.6％。 相比之下，使用COCO数据集重复相同的实验，得出的答案肯定要少得多（34.5％）。 对于第二个问题，我们进行了一个实验，实验的问题是“您认为这是某人可能为这张图片给出的现实而合理的情感回应吗？”。 给定随机采样的话语，用户可以选择四个选项，以指示该作品的响应适当程度。 我们在Supp中详细说明了结果。 垫。; 总的来说，97.5％的话语被认为是适当的。 为了回答最后一个问题，我们为Turkers提供了一件艺术品，并附上其附带的解释之一，并将其以随机和并排的顺序并排放置在两幅随机艺术品的旁边。 我们要求Turkers在给定的解释中猜测“推荐”的艺术品。 在总共1K次试验中，Turkers成功地预测了“目标”绘画的时间为94.7％。

这些发现表明，尽管ArtEmis具有固有的主观性，但在确定合理的情感话语方面存在着重要的共同点，并建议旨在构建可复制此类高质量字幕的模型。



### 4. Neural methods

#### 4.1 Auxiliary classification tasks

在介绍neural speakers之前，我们先介绍两个辅助分类问题和相应的基于神经的解决方案。 首先，我们提出了用ArtEmis的**给定文本解释**来预测所预测的情绪的问题。 这是<u>经典的9向文本分类问题</u>，允许采用标准解决方案。 在我们的实现中，<u>我们将基于交叉熵的优化应用于从头开始训练的LSTM文本分类器，并且还考虑将预训练的BERT模型微调至此任务[20]。</u>

其次，我们提出了预测用户通常会给予艺术品的预期情绪反应分布的问题。 为了解决这个问题，我们通过最小化其输出与ArtEmis的经验用户分布之间的KL-divergence来微调基于ImageNet的[18]预训练的ResNet32编码器[26]。

可以使用我们分别表示为C情感和C情感的这两个分类器对我们的神经说话者非常有用，因为我们可以使用它们来评估并引导其输出的情感内容（第5节） 和4.2）。 当然，这两个问题也具有内在价值，我们将在第6节中详细探讨它们。

